\chapter{Introduction}
\label{chapterlabel1}

Real time bidding (RTB) has recently become paradigm for online advertising which subversively transformed the whole ecosystem. Unlike traditional contextual advertising
RTB allows advertisers to bid for each of the impression based on user profile data, but not the contextual web data. Demand side platform (DSP)'s responsibility is to help advertisers optimize their bidding strategy in order to maximize the return of investment (ROI) for its clients. When a potential advertisement audience visits a webpage, a bid request will be sent to DSP who will decide a price to bid for the webspace slot. Economically we know that \(Profit = Revenue - Cost\), in which cost is the winning price for the advertiser, called \textit{Cost-Per-Click} and revenue is the expected return that can be obtained from the potential advertisement audience based on auction winning function which can be found in \cite{zhang2014optimal}. To be simplified, the revenue can be yielded by the product of the expected click per impression and the revenue that one click can bring. The pursuit of maximum profit can be achieved by either increasing revenue or decreasing the cost, accurate CTR prediction is crucial to this process which determines the expected potential revenue for the impression and according bidding price. 
CTR estimation has been researched intensively in recent years academically and industrially, it is especially important to the industry since the CTR prediction impacts the user experience and advertiser revenue. Microsoft \cite{graepel2010web}has proposed a novel way for CTR prediction on Sponsored Search in Microsoftâ€™s Bing search engine, Gaussian beliefs over weights of the model is maintained and the weights are updated online based on approximate message passing. Facebook \cite{he2014practical} also demonstrates their success on online advertisement CTR prediction. With the combination of decision trees with logistic regression, and other techniques such as feature selection, data freshness, learning rate schema and data sampling, the performance can be increased. However, most current work are restricted to the scope of enhancement of model formation, parameter adjustment, feature engineering, but according to facebook recent report \cite{facebook2015}, the \textit{experimental paradigm is reaching its limits}, as an experimental science, a single experimental paradigm, which is (i) Setting aside test dataset, (ii) Estimating prediction function using training dataset, and (iii) Measure final performance using testing dataset. However, for such an experiment, selection of data is the most crucial part, the data distribution has to be matched with the operational conditions. So in order to improve the performance of machine learning model, better data is more improtant than better algorithm and better features.

However, in the age of big data \cite{lohr2012age}, large dataset are collected automatically so bias is inevitable among datasets which results in the unrealistic results from training /testing using biased sets \cite{torralba2011unbiased}. So for current research most studies are done based on one single dataset, so the classifiers obtained can only works statistically under a certain distributions. Only if with the updates of the model, can the classifiers be applied to new campaigns in the concept of online advertisement. 

Out ambition in this paper is to admitting the existence of the bias among datasets, to formulate counting feature based on the statistical property of the dataset. The counting feature of each field in the meta data is composed of two types, i.e., frequency feature and average CTR feature. Frequency feature represents the distribution of the items in the field, and average CTR feature shows the distribution the clicks of the items in the field. For example, for the field of \textit{City} in meta data there are 100 different cities, if Beijing appears 100,000 times in the dataset with a total number of 1,000,000 impressions, which has the click number 100, and Hong Kong appears 50,000 times, also with the click number 100. Then for the concept of counting feature, there will be only two feature generated from the field \textit{city} in the metadata, and the counting values are continuous such as Beijing with frequency counting value 0.1 and average CTR counting value 0.001. But for binary feature which is built into one-hot feature, there will be 100 feature generated from the field \textit{city} with only one feature as 1 and others as 0 for each impression, which is sparse and redundant. Theoretically, the performance of counting feature in non-linear logistic regression model is comparable to that of binary feature in linear logistic regression model, with the combination of boosted decision tree and logistic regression, counting feature can achieve the same performance in CTR prediction compared to binary feature but with fixed number of feature which is the double of that of the number of the fields in the meta data. Generally, model features are widely used in linear regression based estimators such as logistic regression. Counting features are used in the tree models such as random forest or gradient boosting regression tree for their continuity property. However, there is no work extensively studying the comparison and relationship of these two kinds of features. Particularly, the literature on counting feature based CTR estimation is very rare.

The classical problem for online learning is so-called \textit{Cold Start} problem, which is how to predict future based on historical information. In the scope of online advertisement CTR prediction, the problem is transformed to with the model of trained from old advertisement campaigns, how to apply it to new campaigns in order to avoid repetitive work and make large-scale industry online implementation possible. Most of the current research such as \cite{mohan2011web} \cite{chu2011unbiased} \cite{he2014practical}\cite{mcmahan2013ad} regard it as an active learning problem, the weights is regarded as a probability distribution and using Bayesian probit method the weights can be updated with the income of new data thus enhance the precision of CTR prediction. However, in the time of big data, online data stream is extremely huge which cost resourse and time for the updating of the model. In this paper, we try to build general cross domain model which can be used for every advertisement campaign without the tedious progress of feature selection and model updating, but remain fixed number of features and variable counting feature values, thus transforming the variability form model to the dataset itself. The value  due to the continuity of counting feature, the value of a certain field can be updated with the increasing amount of statistical information from the new dataset. We prove that counting feature with low dimensions performs significantly better than binary feature with high dimensions in terms of cross-domain CTR prediction, the experiments show that with the increasing volume of statistical information gathered from new campaign, the AUC increases accordingly for counting feature model but no impact on binary feature model. 

In summary, the contribution of this paper is as follows:

\begin{itemize}
\item We find the relation between binary feature space and counting feature space and shows that their performances for CTR estimatino are comparable
\item We analyses the model similarity problem and classify the relation between two campaigns as three types, and we also show that the performance of counting feature compared to binary feature is decided by the distribution of the campaign itself and the relation between old train campaign and new campaign.
\item We show the performance of counting feature in cross-domain learning problem for CTR prediction compared to binary feature and validate that counting feature's performance is significantly better than binary feature
\end{itemize}

The rest of the paper is organised as follows, section 2 discusses the related work, our justification of counting feature is formulated in section 3 and in section 4 we discuss on the cross-domain problem for online advertisement and model generalization, experiments are shown in section 5 and we conclude in the last section.

%Inline citation: \bibentry{example-citation}

% This just dumps some pseudolatin in so you can see some text in place.

